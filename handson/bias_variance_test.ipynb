{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias-Variance Tradeoff\n",
    "Assume that we have noisy data, modeled by $f = y + \\epsilon$, where $\\epsilon \\in \\mathcal{N}(0,\\sigma)$. Given an estimator $\\hat{f}$, the squared error can be derived as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{E}\\left[\\left(\\hat{f} - f\\right)^2\\right] &= \\mathbb{E}\\left[\\hat{f}^2 - 2f\\hat{f} + f^2\\right]\\\\\n",
    "&= \\mathbb{E}\\left[\\hat{f}^2\\right] + \\mathbb{E}\\left[f^2\\right] - 2\\mathbb{E}\\left[f\\hat{f}^2\\right] \\text{ By linearity of expectation} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "Now, by definition, $Var(x) = \\mathbb{E}\\left[x^2\\right] - \\left(\\mathbb{E}\\left[x\\right]\\right)^2$. Subsituting this definition into the eqaution above, we get:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{E}\\left[\\hat{f}^2\\right] + \\mathbb{E}\\left[f^2\\right] - 2\\mathbb{E}\\left[f\\hat{f}^2\\right] &= Var(\\hat{f}) + \\left(\\mathbb{E}[\\hat{f}]\\right)^2  + Var(f) + \\left(\\mathbb{E}[f]\\right)^2 - 2f\\mathbb{E}[\\hat{F}^2] \\\\ \n",
    "&= Var(\\hat{f}) + Var(f) + \\left(\\mathbb{E}[\\hat{f}] - f\\right)^2\\\\\n",
    "&= \\boxed{\\sigma + Var(\\hat{f}) + \\left(\\mathbb{E}[\\hat{f}] - f\\right)^2}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The first term $\\sigma$ is the irreducible error due to the noise in the data (from the distribution of $\\epsilon$). The second term is the **variance** of the estimator $\\hat{f}$ and the final term is the **bias** of the estimator. There is an inherent tradeoff between the bias and variance of an estimator. Generally, more complex estimators (think of high-degree polynomials as an example) will have a low bias since they will fit the sampled data really well. However, this accuracy will not be maintained if we continued to resample the data, which implies that the variance of this estimator is high.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity\n",
    "We will now see how the Bias and Variance of an estimator change using an SVM. Consider the dataset generated by the code below (two gaussians). We will be changing the misclassification cost and observing the resulting changes in the hyperplane that tries to separate the two clusters of points. Intuitively, we can expect that with a higher cost, i.e. more severe penalty to misclassifications, the separating hyperplane would fit the data more closely, thereby resulting in low bias. At the same time, if we resampled the data, the hyperplane would no longer be quite as accurate in separatin the points due to higher variance. We plot the bias and the variance of the estimator for a range of costs and take note of the genral trends of these values below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import svm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#SVM kernel - linear, poly, rbf, etc.\n",
    "kernel='rbf'\n",
    "\n",
    "#Cost values\n",
    "Cs = [1,2,5,10,20,50,100,200,500,1000]\n",
    "\n",
    "#define data\n",
    "n = 100\n",
    "sub = 10\n",
    "var = 2\n",
    "mean = 1\n",
    "mean1 = [mean, mean]\n",
    "cov1 = [[2*var, 0], [0, 2*var]]\n",
    "mean2 = [-mean, -mean]\n",
    "cov2 = [[var,0],[0,var]]\n",
    "g = 8\n",
    "\n",
    "def plot_boundary(clf,g):\n",
    "    #adapted from http://scikit-learn.org/stable/modules/svm.html\n",
    "    h = 0.2\n",
    "    xx, yy = np.meshgrid(np.arange(-g,g, h),np.arange(-g,g, h))    \n",
    "    Z = svc.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)\n",
    "\n",
    "#sample dataset of two gaussians\n",
    "x1, y1 = np.random.multivariate_normal(mean1, cov1, n).T\n",
    "x2, y2 = np.random.multivariate_normal(mean2, cov2, n).T\n",
    "Xtest = np.concatenate((np.asarray([x1,y1]).T,np.asarray([x2,y2]).T),axis=0)\n",
    "ytest = np.asarray([-1]*(n) + [1]*(n))\n",
    "\n",
    "#plot current sample\n",
    "plt.scatter(x1,y1,color='red')\n",
    "plt.scatter(x1,y2, color='blue')\n",
    "plt.show()\n",
    "\n",
    "biasSquared = np.zeros(sub)\n",
    "preds = np.zeros((2*n,sub))\n",
    "\n",
    "b = np.zeros(len(Cs))\n",
    "v = np.zeros(len(Cs))\n",
    "\n",
    "for j,C in enumerate(Cs):\n",
    "    for i in range(sub):\n",
    "        \n",
    "        #create data - 2D Gaussians     \n",
    "        x1, y1 = np.random.multivariate_normal(mean1, cov1, n).T\n",
    "        x2, y2 = np.random.multivariate_normal(mean2, cov2, n).T\n",
    "        X = np.concatenate((np.asarray([x1,y1]).T,np.asarray([x2,y2]).T),axis=0)\n",
    "        y = np.asarray([-1]*(n) + [1]*(n))\n",
    "        \n",
    "        #fit SVM\n",
    "        svc = svm.SVC(kernel=kernel,C=C).fit(X,y)\n",
    "        preds[:,i] = svc.predict(Xtest)\n",
    "        \n",
    "        #plot \n",
    "        if i < 4:\n",
    "            plt.subplot(2,2,i+1)\n",
    "            plot_boundary(svc,g)\n",
    "            plt.plot(x1, y1, 'b.', x2, y2, 'r.')\n",
    "\n",
    "    plt.axis([-g,g,-g,g])\n",
    "    plt.suptitle('Cost = %i' % (C))\n",
    "    plt.show()\n",
    "    b[j] = np.mean(np.mean(preds,1) - ytest) \n",
    "    v[j] = np.mean(np.var(preds,1))\n",
    "\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(Cs,b)\n",
    "plt.title('bias')\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(Cs,v)\n",
    "plt.title('variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
